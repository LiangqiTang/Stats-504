{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quantization and support points\n",
    "\n",
    "See the [multivariate methods document](https://github.com/kshedden/case_studies/blob/main/methods/multivariate.md) for an overview of quantization.\n",
    "The \"support points\" method used in this notebook was originally proposed in this paper: https://arxiv.org/pdf/1609.01811.pdf.  You don't need to review this paper, as sufficient background is provided in this notebook, in the multivariate methods document, and in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from read import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Subsample the profiles for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 20000\n",
    "n = temp.shape[1]\n",
    "ii = np.random.choice(n, m, replace=False)\n",
    "temp = temp[:, ii]\n",
    "psal = psal[:, ii]\n",
    "lat = lat[ii]\n",
    "lon = lon[ii]\n",
    "day = day[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The function below randomly subsamples columns of X, by default taking 2000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsamp(X, cmax=2000):\n",
    "    \"\"\"\n",
    "    Subsample the columns of X so that there are more than 'cmax' columns.\n",
    "    \"\"\"\n",
    "    if X.shape[1] > cmax:\n",
    "        ii = np.random.choice(X.shape[1], cmax, replace=False)\n",
    "        X = X[:, ii]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The function below calculates the average pairwise distance between a column of X and a column of Y.  This is an important component of the energy distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ediff(X, Y):\n",
    "    \"\"\"\n",
    "    Calculate the average L2 distance between a column of X and a column of Y.\n",
    "    \"\"\"\n",
    "    if X.shape[1] > Y.shape[1]:\n",
    "        X, Y = Y, X\n",
    "    d = 0.0\n",
    "    for j in range(X.shape[1]):\n",
    "        u = Y - X[:, j][:, None]\n",
    "        d += np.sqrt((u**2).sum(0)).sum()\n",
    "    d /= (X.shape[1] * Y.shape[1])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "It will be helpful to know the \"diameters\" of the temperature and salinity profile sets, which can be calculated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = dsamp(temp)\n",
    "ediff(temp1, temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "psal1 = dsamp(psal)\n",
    "ediff(psal1, psal1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The function below estimates the energy distance between the distributions of the columns of X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_distance(X, Y):\n",
    "    \"\"\"\n",
    "    Estimate the energy distance between P(X) and P(Y).\n",
    "    \"\"\"\n",
    "    X = dsamp(X)\n",
    "    Y = dsamp(Y)\n",
    "    return 2*ediff(X, Y) - ediff(X, X) - ediff(Y, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The function below is a utility function, used in the majorization/maximization (MM) algorithm for constructing support points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Equation 22 in Mak et al.\n",
    "def update_support(X, Y):\n",
    "    N, p = Y.shape\n",
    "    n, _ = X.shape\n",
    "    XX = np.zeros((n, p))\n",
    "\n",
    "    for i in range(n):\n",
    "        Dx = X[i, :] - X\n",
    "        DxN = np.linalg.norm(Dx, axis=1)\n",
    "        DxN[i] = np.inf\n",
    "        Dy = X[i, :] - Y\n",
    "        DyN = np.linalg.norm(Dy, axis=1)\n",
    "        q = (1/DyN).sum()\n",
    "        XX[i, :] = np.dot(1/DxN, Dx) * (N / n)\n",
    "        XX[i, :] += np.dot(1/DyN, Y)\n",
    "        XX[i, :] /= q\n",
    "\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The function below uses the MM algorithm to construct N support points representing the distribution of the columns of Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def support(Y, N, maxiter=1000):\n",
    "    \"\"\"\n",
    "    Calculate N support points for the data in Y.  The points are stored in the rows of Y.\n",
    "    \"\"\"\n",
    "    n, p = Y.shape\n",
    "    X = np.random.normal(size=(N, p))\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        X1 = update_support(X, Y)\n",
    "        ee = np.linalg.norm(X1 - X)\n",
    "        X = X1\n",
    "        if ee < 1e-8:\n",
    "            break\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The support point algorithm is somewhat slow, so we limit the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "support_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_support_map(ii, title):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(8, 7.25))\n",
    "    ax = plt.axes([0.05, 0.05, 0.84, 0.88], projection=ccrs.PlateCarree(central_longitude=180))\n",
    "    ax.coastlines()\n",
    "    ax.set_extent([115, 290, -70, 60])\n",
    "\n",
    "    for j in range(ii.max() + 1):\n",
    "        jj = np.flatnonzero(ii == j)\n",
    "        plt.scatter(lon[jj], lat[jj], s=8, label=str(1+j), transform=ccrs.Geodetic(), rasterized=True)\n",
    "\n",
    "    ha,lb = plt.gca().get_legend_handles_labels()\n",
    "    leg = plt.figlegend(ha, lb, loc=\"center right\")\n",
    "    leg.draw_frame(False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Find the position of the closest support point in the rows of S to the vectors in the rows of X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def support_neighbor(X, S):\n",
    "    ii = np.zeros(X.shape[1]).astype(int)\n",
    "    for i in range(X.shape[1]):\n",
    "        d = ((X[:, i] - S)**2).sum(1)\n",
    "        ii[i] = np.argmin(d)\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Plot support points for temperature and salinity separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "spt = {} # Save the support points for use later\n",
    "\n",
    "for (j,x) in enumerate([temp, psal]):\n",
    "    spt[j] = {}\n",
    "    \n",
    "    # Make plots with different numbers of support points.\n",
    "    for npt in [1, 5, 10]:\n",
    "        print(\"npt=\", npt)\n",
    "        X = support(x.T, npt, maxiter=support_iter)\n",
    "        spt[j][npt] = X\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(6.4,4.8))\n",
    "        plt.grid(True)\n",
    "        plt.title(\"%d support points\" % npt)\n",
    "        for i in range(npt):\n",
    "            plt.plot(pressure, X[i, :], \"-\")\n",
    "        plt.xlabel(\"Pressure (depth)\", size=15)\n",
    "        plt.ylabel([\"Temperature\", \"Salinity\"][j], size=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Plot support points for the combined temperature and salinity trajectories.  Normalize the ranges of temperature and salinity so that the support points are more equally determined by the two measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = matplotlib.colormaps[\"tab10\"]\n",
    "tempz = (temp - temp.mean()) / temp.std()\n",
    "psalz = (psal - psal.mean()) / psal.std()\n",
    "pt = np.vstack([tempz, psalz])\n",
    "for npt in 3,5:\n",
    "    print(\"npt=\", npt)\n",
    "    X = support(pt.T, npt, maxiter=support_iter)\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(6.4,4.8))\n",
    "    plt.axes([0.1, 0.1, 0.78, 0.8])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"%d support points\" % npt)\n",
    "    ax1 = plt.gca()\n",
    "    for i in range(npt):\n",
    "        ax1.plot(pressure, X[i, 0:100], \"-\", color=cm(i/10))\n",
    "    ax1.set_ylabel(\"Temperature (solid lines)\", size=15)\n",
    "    ax2 = ax1.twinx()\n",
    "    for i in range(npt):\n",
    "        ax2.plot(pressure, X[i, 100:200], \":\", color=cm(i/10))\n",
    "    ax2.set_ylabel(\"Salinity (broken lines)\", size=15)\n",
    "    ax1.set_xlabel(\"Pressure (depth)\", size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Make maps showing the distribution of points falling closest to each support point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "npt = 10\n",
    "for j,x in enumerate([temp, psal]):\n",
    "    S = spt[j][npt]\n",
    "    ii = support_neighbor(x, S)\n",
    "    plot_support_map(ii, [\"Temperature\", \"Salinity\"][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Stability analysis of support point estimation\n",
    "\n",
    "One of the principles of [veridical data science](https://vdsbook.com) is *stability*, which basically means that the results should not be highly sensitive to appropriate small data perturbations or reasonable alternatives to the data analysis methods that are employed.  Below we conduct a very simplistic stability assessment for the support point analysis.  We will perturb the data by subsampling 10,000 observations at a time without replacement.  We then quantify the extent to which the minimized energy distance found through the support point optimization changes substantially upon subsampling.  In addition, we assess the extent to which the support points themselves are stable upon conducting this subsampling. \n",
    "\n",
    "\n",
    "Recall from above that the average Euclidean distance between two temperature profiles is around 40.8 and the average Euclidean distance between two salinity profiles is 4.8.  These numbers will be helpful in interpreting the stability findings below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Below we calculate 'npt' support points 'nrep' times, each time using a random subsample of 10,000 profiles.  Set the variable 'x' to either 'temp' or 'psal' to conduct this stability analysis on the temperature or salinity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "npt = 10\n",
    "nrep = 5\n",
    "x = temp\n",
    "S = []\n",
    "for _ in range(nrep):\n",
    "    ii = np.random.choice(temp.shape[1], 10000, replace=False)\n",
    "    X = support(x[:, ii].T, npt, maxiter=support_iter)\n",
    "    S.append(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "One way to assess stability is to check the energy distance between each set of support points and the target distribution.  This is the quantity being minimized when the support points are constructed.  This value will vary among the random subsamples as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = [energy_distance(x, s.T) for s in S]\n",
    "di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "We can compare these energy distances to the distances between a random subset of profiles (of the same size) and the full dataset.  This analysis shows that the 'm' support points do a much better job of representing the population than a random set of 'm' profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    ii = np.random.choice(x.shape[1], npt, replace=False)\n",
    "    x1 = x[:, ii[0:npt]]\n",
    "    print(energy_distance(x1, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We can also consider the energy distance between two sets of support points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = []\n",
    "for i in range(len(S)):\n",
    "    for j in range(i):\n",
    "        di.append(energy_distance(S[i].T, S[j].T))\n",
    "di "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The results above indicate that the distance between two sets of support points is generally greater than the distance from either of them to the data that they are trained to represent.  This suggests that the support points are not especially \"stable\".  This is not necessarily a problem since all of the support point sets that the algorithm finds perform approximately equally well at representing the population of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "How do we know whether the numbers above should be considered to be \"big\"?  One approach is to consider the energy distances between pairs of random samples of profiles, of the same size as the support point sets.  These distances turn out to be much larger and more variable than the distances between sets of support points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    ii = np.random.choice(temp.shape[1], 2*npt, replace=False)\n",
    "    x1 = x[:, ii[0:npt]]\n",
    "    x2 = x[:, ii[npt:2*npt]]\n",
    "    print(energy_distance(x1, x2))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
